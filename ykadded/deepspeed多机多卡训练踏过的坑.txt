deepspeed多机多卡训练踏过的坑

https://zhuanlan.zhihu.com/p/624223085

deepspeed多机多卡训练踏过的坑
100110
100110
huggGPT
​关注他

你关注的 海狸同学 赞同
租了阿里云的两台GPU云服务器。想对一个10B的模型进行finetune，不想使用peft的LoRA或者Ptuning。就要全参数微调，然后发现，单机的四张GPU跑不动，虽然四张总和近100GB显存，跑100亿的参数还是不行。然后想两台一起跑，总共200GB显存，batch_size=1。这样应该能跑地动。

然后想到了微软的deepspeed工具。文档是真地少啊。下面的是huggingface官网的

DeepSpeed Integration

通过它单机多卡非常好配置，其中一个配置如下：

gpu_vis=0,1,2,3
MASTER_PORT=2345
deepspeed  --include localhost:$gpu_vis --master_port $MASTER_PORT $base_dir/train.py \
    --deepspeed dspd.json \
    --do_train \
    --train_file $data_train \
    --test_file $data_dev \
按需修改上述参数即可。

然后huggingface的多机多卡脚本：

deepspeed --num_gpus 8 --num_nodes 2 --hostfile hostfile --master_addr hostname1 --master_port=9901 \
your_program.py <normal cl args> --deepspeed ds_config.json
按照上述试了下，完全不起作用。。。。。。。。。

接下来讲述怎么配置多机多卡环境，以下步骤在两个机器上同步进行

首先在编辑两个服务器的/etc/hosts文件，加入node信息

vim /etc/hosts

124.23.134.178 server1
124.32.143.156 server2
然后生成sshkey：

ssh-keygen -t rsa
互相拷贝sshkey

ssh-copy-id root@124.23.134.178
ssh-copy-id root@124.32.143.156
然后本机的sshkey也要拷贝到自己的authorized_keys中

然后试试互相ssh，各自机器上试试:

ssh root@localhost
无密码输入要求时，说明配置成功。

接下来，运行多机多卡脚本，先配置hostfile文件，格式如下：

server1 slots=4
server2 slots=4
第一列尾/etc/hosts里面加的主机node名称，slots后面数字代表，机器上有几张显卡。

然后把代码和模型，在两个机器上都放好，确保路径都一样，然后在其中的一台机器上执行下面脚本

deepspeed --hostfile hostfile \
your_program.py \
--deepspeed ds_config.json
<args>
多机多卡训练，一些错误处理：

1、RuntimeError: Ninja is required to load C++ extensions

从报错来看，是缺少ninja，但是ninja已经安装了，这时候先找到ninja的安装路径。

在训练代码的开头加入：

local_env = os.environ.copy()
local_env["PATH"]="/data/.conda/env/llms/bin:" + local_env["PATH"]
os.environ.update(local_env)
2、运行的时候卡死在（让人头秃几乎一天了。。。。）：

Using /home/nlp/.cache/torch_extensions/py39_cu118 as PyTorch extensions root.....

或者这句话的前后某些地方

需要清理.cache/torch_extensions，把这个文件夹里面的东西都删除掉即可，torch重新程序运行时候会重新生成，如果不删除这个文件夹里面的东西，更换了deepspeed或者transformer后，再次运行训练程序，就会卡死在这个地方

3、单机多卡，多机多卡训练，GPU0上莫名其妙多出来3个进程，每个进程占用1G左右显存，看样子是这三个进程本来应该去GPU1，GPU2，GPU3上，但是没有去成。最后检查发现，今天手残，git pull，把transformers的代码拉到最新，然后用了4.29.0-dev了。更换transformers的版本为稳定版本4.28.1，然后在执行2里面提到的步骤，清除torch缓存，再次执行，发现可以正常运行，显存正常分配

4、训练完毕，改成--do_predict。发现deepspeed默认的Zerostage 2不行，提示使用Zerostage3，换成stage3又一堆问题。直接使用torchrun进行分布式inference。使用torchrun后又出现下面报错：

ValueError: expected sequence of length 109 at dim 1 (got 110)

训练的时候，并没有这样的错误。然后找了许久，可能的原因是inference阶段的数据太小，batch size和训练的的不能一样，得设置小一些，比如我训练batch_size=16，现在inference阶段batch_size=1，目前可以正常inference，但是速度太慢了。

这里有一个解决方案：

https://stackoverflow.com/questions/71166789/huggingface-valueerror-expected-sequence-of-length-165-at-dim-1-got-128

把inference分支的tokenize代码从：

tokenizer(examples['text'], padding=True, truncation=True, max_length=max_length)
改成：

tokenizer(examples['text'], padding='max_length', truncation=True, max_length=max_length)
编辑于 2023-04-27 12:18・IP 属地陕西