deepspeed pytorchå•æœºå¤šå¡ pytorchå¤šå¡å¹¶è¡Œ
https://blog.51cto.com/u_16099237/6323612


deepspeed pytorchå•æœºå¤šå¡ pytorchå¤šå¡å¹¶è¡Œ è½¬è½½
mob6454cc6ba5a52023-05-22 13:42:56
æ–‡ç« æ ‡ç­¾pytorchåˆ†å¸ƒå¼æ·±åº¦å­¦ä¹ æ•°æ®æ•°æ®é›†æ–‡ç« åˆ†ç±»PyTorchäººå·¥æ™ºèƒ½é˜…è¯»æ•°8261



ä¸€ã€DDPå®ç°åˆ†å¸ƒå¼å¹¶è¡Œè®­ç»ƒè¦æ‹¬
Pytorchå¹¶è¡Œä¸»è¦æœ‰ä¸¤ç§æ–¹å¼ï¼ŒDataParallelï¼ˆDPï¼‰å’ŒDistributedDataParallelï¼ˆDDPï¼‰ã€‚DPæ–¹å¼è¾ƒä¸ºç®€å•ï¼Œä½†æ˜¯å¤šçº¿ç¨‹è®­ç»ƒï¼Œå¹¶ä¸”ä¸»å¡æ˜¾å­˜å ç”¨æ¯”å…¶ä»–å¡ä¼šå¤šå¾ˆå¤šã€‚å› æ­¤è¿™é‡Œé‡‡ç”¨DDPæ–¹å¼æ¥å¤šå¡è®¡ç®—ã€‚

DDPæ˜¯å¤šè¿›ç¨‹ï¼Œå°†æ¨¡å‹å¤åˆ¶åˆ°å¤šå—å¡ä¸Šè®¡ç®—ï¼Œæ•°æ®åˆ†é…è¾ƒå‡è¡¡ã€‚

 1 ä½¿ç”¨DDPçš„ä¸€æœºå¤šå¡é…ç½®
1. åŠ å…¥local_rankå‚æ•°ï¼Œè¿™ä¸€æ­¥ä¼šåœ¨ä»£ç è¿è¡Œæ—¶é€šè¿‡torch.distributed.launchè¾“å…¥ï¼Œè¯¥å‚æ•°æ„ä¹‰æ˜¯å½“å‰è¿›ç¨‹æ‰€ç”¨çš„æ˜¯å“ªå—å¡ï¼š

parser.add_argument('--local_rank', default=-1, type=int, help='node rank for distributed training')
1.
2. åˆå§‹åŒ–ï¼Œè®¾ç½®pytorchæ”¯æŒçš„é€šè®¯åç«¯ï¼ŒGPUçš„é€šè®¯é‡‡ç”¨ncclï¼š

torch.distributed.init_process_group(backend="nccl")
1.


3. æ¨¡å‹å°è£…

model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True)
1.
4. æ•°æ®åˆ†é…

train_sampler = DistributedSampler(train_dataset)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=True, sampler=train_sampler)
1.
2.
åœ¨å®šä¹‰DataLoaderçš„æ—¶å€™ï¼Œè¦æŠŠshuffleè®¾ç½®ä¸ºFalseï¼Œå¹¶è¾“å…¥é‡‡æ ·å™¨sampler.

æ•°æ®åˆ†é…åŠ è½½å»¶ä¼¸:

æˆ‘ä»¬é¦–å…ˆè¦çœ‹çœ‹åˆ†å¸ƒå¼æ•°æ®åˆ†é…åŠ è½½çš„æ€»ä½“ç»“æ„ã€‚ç»™å‡ºç¤ºä¾‹ä»£ç ï¼Œå¯ä»¥çœ‹åˆ°ä¸»è¦ä½¿ç”¨äº† DataSet, DistributedSamplerï¼ŒDataLoader è¿™ä¸‰ä¸ªå®ä½“ã€‚

sampler = DistributedSampler(dataset) if is_distributed else None
loader = DataLoader(dataset, shuffle=(sampler is None), sampler=sampler)
for epoch in range(start_epoch, n_epochs):
	if is_distributed:
        sampler.set_epoch(epoch)
        train(loader)
1.
2.
3.
4.
5.
6.
è¿™ä¸‰ä¸ªæ¦‚å¿µçš„é€»è¾‘å…³ç³»å¦‚ä¸‹ï¼š

Dataset : ä»åå­—å¯ä»¥çŸ¥é“ï¼Œæ˜¯æ•°æ®é›†çš„æ„æ€ã€‚è´Ÿè´£å¯¹åŸå§‹è®­ç»ƒæ•°æ®çš„å°è£…ï¼Œå°†å…¶å°è£…æˆ Python å¯è¯†åˆ«çš„æ•°æ®ç»“æ„ï¼ŒDatasetçš„æ´¾ç”Ÿç±»å¿…é¡»æä¾›æ¥å£ä¸€è¾¹è·å–å•ä¸ªæ•°æ®ã€‚
Sampler : ä»åå­—å¯çŸ¥ï¼Œæ˜¯é‡‡æ ·å™¨ï¼Œè´Ÿè´£é‡‡æ ·æ–¹å¼æˆ–è€…è¯´æ˜¯é‡‡æ ·ç­–ç•¥ï¼Œå®ç°æŸç§æå–/é‡‡æ ·ç­–ç•¥ä»Datasetä¹‹ä¸­æ‹¿åˆ°æ•°æ®ç´¢å¼•ï¼Œä¾›DataLoadeä½¿ç”¨ã€‚å¯ä»¥è®¤ä¸ºï¼ŒSampler æ˜¯æŒ‡æŒ¥è€…ï¼Œè´Ÿè´£å†³å®šæˆ˜æ–—åœ¨å“ªé‡Œå¼€å±•ã€‚
DataLoader : è´Ÿè´£ä¾æ®ç´¢å¼•æ¥ä»æ•°æ®é›†ä¸­åŠ è½½æ•°æ®ã€‚æ”¯æŒ Map-style å’Œ Iterable-style ä¸¤ç§Datasetï¼Œæ”¯æŒå•è¿›ç¨‹/å¤šè¿›ç¨‹åŠ è½½ã€‚Loader å°±æ˜¯å…·ä½“ä½œæˆ˜çš„æ–—å£«ï¼Œè´Ÿè´£æŒ‰ç…§ Samplerçš„å‘½ä»¤è¿›è¡Œæˆ˜æ–—ã€‚
å…·ä½“å¦‚ä¸‹å›¾ï¼Œç®€è¦è¯´å°±æ˜¯ï¼š

DataSet æŠŠæ•°æ®é›†æ•°ç›®å‘ç»™DistributedSamplerã€‚
Sampler æŒ‰ç…§æŸç§è§„åˆ™å‘é€æ•°æ®indicesç»™Loaderã€‚
Loader ä¾æ®indicesåŠ è½½æ•°æ®ã€‚
Loader æŠŠæ•°æ®å‘ç»™æ¨¡å‹ï¼Œè¿›è¡Œè®­ç»ƒã€‚
+------------------------+                     +-----------+
|DistributedSampler      |                     |DataLoader |
|                        |     2 indices       |           |
|    Some strategy       +-------------------> |           |
|                        |                     |           |
|-------------+----------|                     |           |
              ^                                |           |  4 data  +-------+
              |                                |       -------------->+ train |
            1 | length                         |           |          +-------+
              |                                |           |
+-------------+----------+                     |           |
|DataSet                 |                     |           |
|        +---------+     |      3 Load         |           |
|        |  Data   +-------------------------> |           |
|        +---------+     |                     |           |
|                        |                     |           |
+------------------------+                     +-----------+
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.
17.
 è¿™é‡Œç®€å•è®²ä¸‹DistributedSampleråˆ°åº•è¦åšä»€ä¹ˆäº‹æƒ…ï¼š

deepspeed pytorchå•æœºå¤šå¡ pytorchå¤šå¡å¹¶è¡Œ_æ•°æ®é›†



å¦‚ä¸Šå›¾ï¼š å‡è®¾å½“å‰æ•°æ®é›†æ€»å…±æœ‰11ä¸ªæ ·æœ¬ï¼ˆ0~10ï¼‰DistributedSampleræ‰§è¡Œæ­¥éª¤ï¼š

Shuffleå¤„ç†ï¼šå°†æ•°æ®é›†çš„æ ·æœ¬éšæœºæ‰“ä¹±ï¼›
æ•°æ®è¡¥å……ï¼šå‡è®¾ç°åœ¨æœ‰ä¸¤å—GPU,å®ƒä¼šå…ˆç”¨11/2å‘ä¸Šå–æ•´=6ï¼Œå†ä¹˜ä»¥GPUçš„ä¸ªæ•°=12ï¼Œå°‘äº†ï¼ˆ12-11ï¼‰=1ä¸ªæ•°æ®ï¼Œæ‰€ä»¥éœ€è¦å¯¹æ•°æ®è¿›è¡Œè¡¥å……ã€‚ä»å¤´å¼€å§‹è¡¥ï¼Œå·®ä¸€ä¸ªå°±æŠŠç¬¬ä¸€ä¸ªæ•°æ®ï¼ˆ6ï¼‰è¡¥ä¸Šï¼Œå·®å‡ ä¸ªè¡¥ä¸Šå‰å‡ ä¸ªæ•°æ®ã€‚è¿™æ ·å°±å¯ä»¥ä¿è¯æ•°æ®å‡è¡¡çš„åˆ†é…åˆ°æ¯ä¸€ä¸ªGPUå½“ä¸­ï¼›
åˆ†é…æ•°æ®ï¼šé—´éš”çš„å°†æ•°æ®åˆ†é…åˆ°å¯¹åº”çš„GPUå½“ä¸­ã€‚
æ„Ÿå…´è¶£çš„ä¹Ÿå¯ä»¥è¯»ä¸‹å®ƒçš„æºç ï¼ˆå¯è¯»å¯ä¸è¯»ï¼‰ï¼š

class DistributedSampler(Sampler):
    def __init__(self, dataset, num_replicas=None, rank=None, shuffle=True, seed=0):
        if num_replicas is None:
            if not dist.is_available():
                raise RuntimeError("Requires distributed package to be available")
            num_replicas = dist.get_world_size()
        if rank is None:
            if not dist.is_available():
                raise RuntimeError("Requires distributed package to be available")
            rank = dist.get_rank()
        self.dataset = dataset  # æ•°æ®é›†
        self.num_replicas = num_replicas  # è¿›ç¨‹ä¸ªæ•° é»˜è®¤ç­‰äºworld_size(GPUä¸ªæ•°)
        self.rank = rank   # å½“å‰å±äºå“ªä¸ªè¿›ç¨‹/å“ªå—GPU
        self.epoch = 0
        self.num_samples = int(math.ceil(len(self.dataset) * 1.0 / self.num_replicas))  # æ¯ä¸ªè¿›ç¨‹çš„æ ·æœ¬ä¸ªæ•°
        self.total_size = self.num_samples * self.num_replicas  # æ•°æ®é›†æ€»æ ·æœ¬çš„ä¸ªæ•°
        self.shuffle = shuffle  # æ˜¯å¦è¦æ‰“ä¹±æ•°æ®é›†
        self.seed = seed

    def __iter__(self):
        # 1ã€ Shuffleå¤„ç†ï¼šæ‰“ä¹±æ•°æ®é›†é¡ºåº
        if self.shuffle:
            # deterministically shuffle based on epoch and seed
            g = torch.Generator()
            # è¿™é‡Œself.seedæ˜¯ä¸€ä¸ªå®šå€¼ï¼Œé€šè¿‡set_epochæ”¹å˜self.epochå¯ä»¥æ”¹å˜æˆ‘ä»¬çš„åˆå§‹åŒ–ç§å­
            # è¿™å°±å¯ä»¥è®©æˆ‘ä»¬åœ¨æ¯ä¸€ä¸ªepochä¸­æ•°æ®é›†çš„æ‰“ä¹±é¡ºåºä¸åŒï¼Œä½¿æˆ‘ä»¬æ¯ä¸€ä¸ªepochä¸­æ¯ä¸€å—GPUæ‹¿åˆ°çš„æ•°æ®éƒ½ä¸ä¸€æ ·ï¼Œè¿™æ ·å¯ä»¥æœ‰åˆ©äºæ›´å¥½çš„è®­ç»ƒ
            g.manual_seed(self.seed + self.epoch)
            indices = torch.randperm(len(self.dataset), generator=g).tolist()
        else:
            indices = list(range(len(self.dataset)))

        # æ•°æ®è¡¥å……
        indices += indices[:(self.total_size - len(indices))]
        assert len(indices) == self.total_size

        # åˆ†é…æ•°æ®
        indices = indices[self.rank:self.total_size:self.num_replicas]
        assert len(indices) == self.num_samples

        return iter(indices)

    def __len__(self):
        return self.num_samples

    def set_epoch(self, epoch):
        r"""
        Sets the epoch for this sampler. When :attr:`shuffle=True`, this ensures all replicas
        use a different random ordering for each epoch. Otherwise, the next iteration of this
        sampler will yield the same ordering.
        Arguments:
            epoch (int): Epoch number.
        """
        self.epoch = epoch
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.
17.
18.
19.
20.
21.
22.
23.
24.
25.
26.
27.
28.
29.
30.
31.
32.
33.
34.
35.
36.
37.
38.
39.
40.
41.
42.
43.
44.
45.
46.
47.
48.
49.
50.
51.
52.
53.
 2 å®Œæ•´çš„ä¾‹å­(main.py)
import argparse
import os
import shutil
import time
import warnings
import numpy as np

warnings.filterwarnings('ignore')

import torch
import torch.nn as nn
import torch.nn.parallel
import torch.backends.cudnn as cudnn
import torch.distributed as dist
import torch.optim
import torch.utils.data
import torch.utils.data.distributed
from torch.utils.data.distributed import DistributedSampler

from models import DeepLab
from dataset import Cityscaples


parser = argparse.ArgumentParser(description='DeepLab')

parser.add_argument('-j', '--workers', default=4, type=int, metavar='N',
                    help='number of data loading workers (default: 4)')
parser.add_argument('--epochs', default=100, type=int, metavar='N',
                    help='number of total epochs to run')
parser.add_argument('--start-epoch', default=0, type=int, metavar='N',
                    help='manual epoch number (useful on restarts)')
parser.add_argument('-b', '--batch-size', default=3, type=int,
                    metavar='N')
parser.add_argument('--local_rank', default=0, type=int, help='node rank for distributed training')


args = parser.parse_args()
torch.distributed.init_process_group(backend="nccl") # åˆå§‹åŒ–


print("Use GPU: {} for training".format(args.local_rank))

# create model
model = DeepLab()

torch.cuda.set_device(args.local_rank) # å½“å‰å¡
model = model.cuda() # æ¨¡å‹æ”¾åœ¨å¡ä¸Š
model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True) # æ•°æ®å¹¶è¡Œ

criterion = nn.CrossEntropyLoss().cuda()

optimizer = torch.optim.SGD(model.parameters(), args.lr, momentum=args.momentum, weight_decay=args.weight_decay)

train_dataset = Cityscaples()
train_sampler = DistributedSampler(train_dataset) # åˆ†é…æ•°æ®

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=True, sampler=train_sampler) #shuffleè¦è®¾ç½®ä¸ºFalse
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.
17.
18.
19.
20.
21.
22.
23.
24.
25.
26.
27.
28.
29.
30.
31.
32.
33.
34.
35.
36.
37.
38.
39.
40.
41.
42.
43.
44.
45.
46.
47.
48.
49.
50.
51.
52.
53.
54.
55.
56.
57.
é€šè¿‡å‘½ä»¤è¡Œå¯åŠ¨ï¼š

CUDA_VISIBLE_DEVICES=0,1 python -m torch.distributed.launch --nproc_per_node=2 main.py
1.
CUDA_VISIBLE_DEVICESè®¾ç½®è¦åˆ©ç”¨çš„GPUå¡å·
--nproc_per_nodeè®¾ç½®å‡ å—å¡
å‚æ•°ä¸­ä¸€å®šè¦åŠ å…¥ --local_rank
äºŒã€è¯¦æƒ…æ±‚çœŸ
å…¶å®å•æœºå¤šå¡çš„åŠæ³•è¿˜æœ‰å¾ˆå¤šï¼š

1ã€nn.DataParallel ç®€å•æ–¹ä¾¿çš„ nn.DataParallel

2ã€torch.distributed ä½¿ç”¨ torch.distributed åŠ é€Ÿå¹¶è¡Œè®­ç»ƒ

3ã€apex ä½¿ç”¨ apex å†åŠ é€Ÿã€‚

è¿™é‡Œï¼Œè®°å½•äº†ä½¿ç”¨ 4 å— Tesla V100-PICE åœ¨ ImageNet è¿›è¡Œäº†è¿è¡Œæ—¶é—´çš„æµ‹è¯•ï¼Œæµ‹è¯•ç»“æœå‘ç° Apex çš„åŠ é€Ÿæ•ˆæœæœ€å¥½ï¼Œä½†ä¸ Horovod/Distributed å·®åˆ«ä¸å¤§ï¼Œå¹³æ—¶å¯ä»¥ç›´æ¥ä½¿ç”¨å†…ç½®çš„ Distributedã€‚Dataparallel è¾ƒæ…¢ï¼Œä¸æ¨èä½¿ç”¨ã€‚

deepspeed pytorchå•æœºå¤šå¡ pytorchå¤šå¡å¹¶è¡Œ_æ•°æ®_02

 1 ä½¿ç”¨nn.DataParallelå¹¶è¡Œè®­ç»ƒ
å…ˆé—®ä¸¤ä¸ªé—®é¢˜ï¼š

é—®1ï¼šä¸ºå•¥éè¦å•æœºå¤šå¡ï¼Ÿ

ç­”1ï¼šåŠ é€Ÿç¥ç»ç½‘ç»œè®­ç»ƒæœ€ç®€å•çš„åŠæ³•å°±æ˜¯ä¸ŠGPUï¼Œå¦‚æœä¸€å—GPUè¿˜æ˜¯ä¸å¤Ÿï¼Œå°±å¤šä¸Šå‡ å—ã€‚äº‹å®ä¸Šï¼Œæ¯”å¦‚BERTå’ŒGPT-2è¿™æ ·çš„å¤§å‹è¯­è¨€æ¨¡å‹ç”šè‡³æ˜¯åœ¨ä¸Šç™¾å—GPUä¸Šè®­ç»ƒçš„ã€‚ä¸ºäº†å®ç°å¤šGPUè®­ç»ƒï¼Œæˆ‘ä»¬å¿…é¡»æƒ³ä¸€ä¸ªåŠæ³•åœ¨å¤šä¸ªGPUä¸Šåˆ†å‘æ•°æ®å’Œæ¨¡å‹ï¼Œå¹¶ä¸”åè°ƒè®­ç»ƒè¿‡ç¨‹ã€‚

é—®2ï¼šä¸Šä¸€ç¯‡è®²å¾—å•æœºå¤šå¡æ“ä½œnn.DataParallelï¼Œå“ªé‡Œä¸å¥½ï¼Ÿ

ç­”2ï¼šè¦å›ç­”è¿™ä¸ªé—®é¢˜æˆ‘ä»¬å¾—å…ˆç®€å•å›é¡¾ä¸€ä¸‹nn.DataParallelï¼Œè¦ä½¿ç”¨è¿™ç©æ„ï¼Œæˆ‘ä»¬å°†æ¨¡å‹å’Œæ•°æ®åŠ è½½åˆ°å¤šä¸ª GPU ä¸­ï¼Œæ§åˆ¶æ•°æ®åœ¨ GPU ä¹‹é—´çš„æµåŠ¨ï¼ŒååŒä¸åŒ GPU ä¸Šçš„æ¨¡å‹è¿›è¡Œå¹¶è¡Œè®­ç»ƒã€‚å…·ä½“æ€ä¹ˆæ“ä½œï¼Ÿæˆ‘ä»¬åªéœ€è¦ç”¨ DataParallel åŒ…è£…æ¨¡å‹ï¼Œå†è®¾ç½®ä¸€äº›å‚æ•°å³å¯ã€‚éœ€è¦å®šä¹‰çš„å‚æ•°åŒ…æ‹¬ï¼š

å‚ä¸è®­ç»ƒçš„ GPU æœ‰å“ªäº›ï¼Œdevice_ids=gpusã€‚
ç”¨äºæ±‡æ€»æ¢¯åº¦çš„ GPU æ˜¯å“ªä¸ªï¼Œoutput_device=gpus[0] ã€‚
DataParallel ä¼šè‡ªåŠ¨å¸®æˆ‘ä»¬å°†æ•°æ®åˆ‡åˆ† load åˆ°ç›¸åº” GPUï¼Œå°†æ¨¡å‹å¤åˆ¶åˆ°ç›¸åº” GPUï¼Œè¿›è¡Œæ­£å‘ä¼ æ’­è®¡ç®—æ¢¯åº¦å¹¶æ±‡æ€»ï¼š

model = nn.DataParallel(model.cuda(), device_ids=gpus, output_device=gpus[0])
1.
 å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ¨¡å‹å’Œæ•°æ®éƒ½éœ€è¦å…ˆ load è¿› GPU ä¸­ï¼ŒDataParallel çš„ module æ‰èƒ½å¯¹å…¶è¿›è¡Œå¤„ç†ï¼Œå¦åˆ™ä¼šæŠ¥é”™ï¼š

# main.py
import torch
import torch.distributed as dist

gpus = [0, 1, 2, 3]
torch.cuda.set_device('cuda:{}'.format(gpus[0]))

train_dataset = ...

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=...)

model = ...
model = nn.DataParallel(model.to(device), device_ids=gpus, output_device=gpus[0])

optimizer = optim.SGD(model.parameters())

for epoch in range(100):
   for batch_idx, (data, target) in enumerate(train_loader):
      images = images.cuda(non_blocking=True)
      target = target.cuda(non_blocking=True)
      ...
      output = model(images)
      loss = criterion(output, target)
      ...
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.
17.
18.
19.
20.
21.
22.
23.
24.
25.
26.
27.
ç¨å¾®è§£é‡Šå‡ å¥ï¼šmodel.to(device)å°†æ¨¡å‹è¿ç§»åˆ°GPUé‡Œé¢ï¼› images.cuda(non_blocking=True)ï¼Œtarget.cuda(non_blocking=True)æŠŠæ•°æ®è¿ç§»åˆ°GPUé‡Œé¢ï¼› nn.DataParallel(model.to(device), device_ids=gpus, output_device=gpus[0])åŒ…è£…æ¨¡å‹ã€‚ images.cuda(non_blocking=True)ä¸ºä½•è¦è®¾ç½®å‚æ•°non_blocking=Trueå‘¢ï¼Ÿ è§£é‡Šï¼šnon_blockingé»˜è®¤å€¼ä¸ºFalse, é€šå¸¸æˆ‘ä»¬ä¼šåœ¨åŠ è½½æ•°æ®æ—¶ï¼Œå°†DataLoaderçš„å‚æ•°pin_memoryè®¾ç½®ä¸ºTrue, DataLoaderä¸­å‚æ•°pin_memoryçš„ä½œç”¨æ˜¯ï¼šå°†ç”Ÿæˆçš„Tensoræ•°æ®å­˜æ”¾åœ¨å“ªé‡Œï¼Œå€¼ä¸ºTrueæ—¶ï¼Œæ„å‘³ç€ç”Ÿæˆçš„Tensoræ•°æ®å­˜æ”¾åœ¨é”é¡µå†…å­˜ä¸­ï¼Œè¿™æ ·å†…å­˜ä¸­çš„Tensorè½¬ä¹‰åˆ°GPUçš„æ˜¾å­˜ä¼šæ›´å¿«ã€‚ ä¸»æœºä¸­çš„å†…å­˜ï¼Œæœ‰ä¸¤ç§å­˜åœ¨æ–¹å¼ï¼Œä¸€æ˜¯é”é¡µï¼ŒäºŒæ˜¯ä¸é”é¡µï¼Œé”é¡µå†…å­˜å­˜æ”¾çš„å†…å®¹åœ¨ä»»ä½•æƒ…å†µä¸‹éƒ½ä¸ä¼šä¸ä¸»æœºçš„è™šæ‹Ÿå†…å­˜è¿›è¡Œäº¤æ¢ï¼ˆæ³¨ï¼šè™šæ‹Ÿå†…å­˜å°±æ˜¯ç¡¬ç›˜ï¼‰ï¼Œè€Œä¸é”é¡µå†…å­˜åœ¨ä¸»æœºå†…å­˜ä¸è¶³æ—¶ï¼Œæ•°æ®ä¼šå­˜æ”¾åœ¨è™šæ‹Ÿå†…å­˜ä¸­ã€‚æ˜¾å¡ä¸­çš„æ˜¾å­˜å…¨éƒ¨æ˜¯é”é¡µå†…å­˜,å½“è®¡ç®—æœºçš„å†…å­˜å……è¶³çš„æ—¶å€™ï¼Œå¯ä»¥è®¾ç½®pin_memory=Trueã€‚å½“ç³»ç»Ÿå¡ä½ï¼Œæˆ–è€…äº¤æ¢å†…å­˜ä½¿ç”¨è¿‡å¤šçš„æ—¶å€™ï¼Œè®¾ç½®pin_memory=Falseã€‚ï¼ˆå‚è€ƒï¼šé“¾æ¥ï¼‰ å¦‚æœpin_memory=Trueçš„è¯ï¼Œå°†æ•°æ®æ”¾å…¥GPUçš„æ—¶å€™ï¼Œä¹Ÿåº”è¯¥æŠŠnon_blockingæ‰“å¼€ï¼Œè¿™æ ·å°±åªæŠŠæ•°æ®æ”¾å…¥GPUè€Œä¸å–å‡ºï¼Œè®¿é—®æ—¶é—´ä¼šå¤§å¤§å‡å°‘ã€‚

 nn.DataParallel çš„ç¼ºç‚¹ï¼š

å•è¿›ç¨‹å¤šçº¿ç¨‹ã€‚åœ¨æ¯ä¸ªè®­ç»ƒæ‰¹æ¬¡ï¼ˆbatchï¼‰ä¸­ï¼Œå› ä¸ºæ¨¡å‹çš„æƒé‡éƒ½æ˜¯åœ¨ä¸€ä¸ªè¿›ç¨‹ä¸Šå…ˆç®—å‡ºæ¥ï¼Œç„¶åå†æŠŠä»–ä»¬åˆ†å‘åˆ°æ¯ä¸ªGPUä¸Šï¼Œæ‰€ä»¥ç½‘ç»œé€šä¿¡å°±æˆä¸ºäº†ä¸€ä¸ªç“¶é¢ˆï¼Œè€ŒGPUä½¿ç”¨ç‡ä¹Ÿé€šå¸¸å¾ˆä½ã€‚
é™¤æ­¤ä¹‹å¤–ï¼Œnn.DataParallel éœ€è¦æ‰€æœ‰çš„GPUéƒ½åœ¨ä¸€ä¸ªèŠ‚ç‚¹ï¼ˆä¸€å°æœºå™¨ï¼‰ä¸Šï¼Œä¸”å¹¶ä¸æ”¯æŒ Apex çš„ æ··åˆç²¾åº¦è®­ç»ƒã€‚
ä¸€å¥è¯æ€»ç»“ï¼šä¸€ä¸ªè¿›ç¨‹ç®—æƒé‡ä½¿é€šä¿¡æˆä¸ºç“¶é¢ˆï¼Œnn.DataParallelæ…¢è€Œä¸”ä¸æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒã€‚
 2 ä½¿ç”¨ torch.distributed åŠnn.DistributedDataParallelåŠ é€Ÿå¹¶è¡Œè®­ç»ƒï¼š
DataParallelï¼šå•è¿›ç¨‹æ§åˆ¶å¤š GPUã€‚

DistributedDataParallelï¼šå¤šè¿›ç¨‹æ§åˆ¶å¤š GPUï¼Œä¸€èµ·è®­ç»ƒæ¨¡å‹ã€‚

2.1 ä»‹ç»
åœ¨ 1.0 ä¹‹åï¼Œå®˜æ–¹ç»ˆäºå¯¹åˆ†å¸ƒå¼çš„å¸¸ç”¨æ–¹æ³•è¿›è¡Œäº†å°è£…ï¼Œæ”¯æŒ all-reduceï¼Œbroadcastï¼Œsend å’Œ receive ç­‰ç­‰ã€‚é€šè¿‡ MPI å®ç° CPU é€šä¿¡ï¼Œé€šè¿‡ NCCL å®ç° GPU é€šä¿¡ã€‚å®˜æ–¹ä¹Ÿæ›¾ç»æåˆ°ç”¨ DistributedDataParallelè§£å†³ DataParallel é€Ÿåº¦æ…¢ï¼ŒGPU è´Ÿè½½ä¸å‡è¡¡çš„é—®é¢˜ï¼Œç›®å‰å·²ç»å¾ˆæˆç†Ÿäº†ã€‚ä¸ DataParallel çš„å•è¿›ç¨‹æ§åˆ¶å¤š GPU ä¸åŒï¼Œåœ¨ distributed çš„å¸®åŠ©ä¸‹ï¼Œæˆ‘ä»¬åªéœ€è¦ç¼–å†™ä¸€ä»½ä»£ç ï¼Œtorch å°±ä¼šè‡ªåŠ¨å°†å…¶åˆ†é…ç»™

ä¸ªè¿›ç¨‹ï¼Œåˆ†åˆ«åœ¨

ä¸ª GPU ä¸Šè¿è¡Œã€‚

deepspeed pytorchå•æœºå¤šå¡ pytorchå¤šå¡å¹¶è¡Œ_pytorch_03

 å’Œå•è¿›ç¨‹è®­ç»ƒä¸åŒçš„æ˜¯ï¼Œå¤šè¿›ç¨‹è®­ç»ƒéœ€è¦æ³¨æ„ä»¥ä¸‹äº‹é¡¹ï¼š

åœ¨å–‚æ•°æ®çš„æ—¶å€™ï¼Œä¸€ä¸ªbatchè¢«åˆ†åˆ°äº†å¥½å‡ ä¸ªè¿›ç¨‹ï¼Œæ¯ä¸ªè¿›ç¨‹åœ¨å–æ•°æ®çš„æ—¶å€™è¦ç¡®ä¿æ‹¿åˆ°çš„æ˜¯ä¸åŒçš„æ•°æ®ï¼ˆDistributedSamplerï¼‰ï¼›
è¦å‘Šè¯‰æ¯ä¸ªè¿›ç¨‹è‡ªå·±æ˜¯è°ï¼Œä½¿ç”¨å“ªå—GPUï¼ˆargs.local_rankï¼‰ï¼›
åœ¨åšBatch Normalizationçš„æ—¶å€™è¦æ³¨æ„åŒæ­¥æ•°æ®ï¼Œå³è¿›è¡Œè·¨å¡åŒæ­¥æ‰¹å½’ä¸€åŒ–ã€‚
2.2 ä½¿ç”¨æ–¹å¼
ï¼ˆ1ï¼‰å¯åŠ¨æ–¹å¼çš„æ”¹å˜

åœ¨å¤šè¿›ç¨‹çš„å¯åŠ¨æ–¹é¢ï¼Œæˆ‘ä»¬ä¸ç”¨è‡ªå·±æ‰‹å†™ multiprocess è¿›è¡Œä¸€ç³»åˆ—å¤æ‚çš„CPUã€GPUåˆ†é…ä»»åŠ¡ï¼ŒPyTorchä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå¾ˆæ–¹ä¾¿çš„å¯åŠ¨å™¨ torch.distributed.launch ç”¨äºå¯åŠ¨æ–‡ä»¶ï¼Œæ‰€ä»¥æˆ‘ä»¬è¿è¡Œè®­ç»ƒä»£ç çš„æ–¹å¼å°±å˜æˆäº†è¿™æ ·ï¼š

CUDA_VISIBLE_DEVICES=0,1,2,3 python \-m torch.distributed.launch \--nproc_per_node=4 main.py
1.
å…¶ä¸­çš„ --nproc_per_node å‚æ•°ç”¨äºæŒ‡å®šä¸ºå½“å‰ä¸»æœºåˆ›å»ºçš„è¿›ç¨‹æ•°ï¼Œç”±äºæˆ‘ä»¬æ˜¯å•æœºå¤šå¡ï¼Œæ‰€ä»¥è¿™é‡Œnodeæ•°é‡ä¸º1ï¼Œæ‰€ä»¥æˆ‘ä»¬è¿™é‡Œè®¾ç½®ä¸ºæ‰€ä½¿ç”¨çš„GPUæ•°é‡å³å¯ã€‚

ï¼ˆ2ï¼‰åˆå§‹åŒ–

åœ¨å¯åŠ¨å™¨ä¸ºæˆ‘ä»¬å¯åŠ¨pythonè„šæœ¬åï¼Œåœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­ï¼Œå¯åŠ¨å™¨ä¼šå°†å½“å‰è¿›ç¨‹çš„ï¼ˆå…¶å®å°±æ˜¯ GPUçš„ï¼‰index é€šè¿‡å‚æ•°ä¼ é€’ç»™ pythonï¼Œæˆ‘ä»¬å¯ä»¥è¿™æ ·è·å¾—å½“å‰è¿›ç¨‹çš„ indexï¼šå³é€šè¿‡å‚æ•° local_rank æ¥å‘Šè¯‰æˆ‘ä»¬å½“å‰è¿›ç¨‹ä½¿ç”¨çš„æ˜¯å“ªä¸ªGPUï¼Œç”¨äºæˆ‘ä»¬åœ¨æ¯ä¸ªè¿›ç¨‹ä¸­æŒ‡å®šä¸åŒçš„deviceï¼š

def parse():
    parser = argparse.ArgumentParser()
    parser.add_argument('--local_rank', type=int, default=0ï¼Œhelp='node rank for distributed training')
    args = parser.parse_args()
    return args

def main():
    args = parse()
    torch.cuda.set_device(args.local_rank)
    torch.distributed.init_process_group(
        'nccl',
        init_method='env://'
    )
    device = torch.device(f'cuda:{args.local_rank}')
    ...
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
å…¶ä¸­ torch.distributed.init_process_group ç”¨äºåˆå§‹åŒ–GPUé€šä¿¡æ–¹å¼ï¼ˆNCCLï¼‰å’Œå‚æ•°çš„è·å–æ–¹å¼ï¼ˆenvä»£è¡¨é€šè¿‡ç¯å¢ƒå˜é‡ï¼‰ã€‚å³ä½¿ç”¨ init_process_group è®¾ç½®GPUä¹‹é—´é€šä¿¡ä½¿ç”¨çš„åç«¯å’Œç«¯å£ï¼Œé€šè¿‡ NCCL å®ç° GPU é€šä¿¡ã€‚

ï¼ˆ3ï¼‰DataLoader

åœ¨è¯»å–æ•°æ®çš„æ—¶å€™ï¼Œæˆ‘ä»¬è¦ä¿è¯ä¸€ä¸ªbatché‡Œçš„æ•°æ®è¢«å‡æ‘Šåˆ°æ¯ä¸ªè¿›ç¨‹ä¸Šï¼Œæ¯ä¸ªè¿›ç¨‹éƒ½èƒ½è·å–åˆ°ä¸åŒçš„æ•°æ®ï¼Œä½†å¦‚æœæˆ‘ä»¬æ‰‹åŠ¨å»å‘Šè¯‰æ¯ä¸ªè¿›ç¨‹æ‹¿å“ªäº›æ•°æ®çš„è¯å¤ªéº»çƒ¦äº†ï¼ŒPyTorchä¹Ÿä¸ºæˆ‘ä»¬å°è£…å¥½äº†è¿™ä¸€æ–¹æ³•ã€‚ä¹‹åï¼Œä½¿ç”¨ DistributedSampler å¯¹æ•°æ®é›†è¿›è¡Œåˆ’åˆ†ã€‚å¦‚æ­¤å‰æˆ‘ä»¬ä»‹ç»çš„é‚£æ ·ï¼Œå®ƒèƒ½å¸®åŠ©æˆ‘ä»¬å°†æ¯ä¸ª batch åˆ’åˆ†æˆå‡ ä¸ª partitionï¼Œåœ¨å½“å‰è¿›ç¨‹ä¸­åªéœ€è¦è·å–å’Œ rank å¯¹åº”çš„é‚£ä¸ª partition è¿›è¡Œè®­ç»ƒã€‚æ‰€ä»¥æˆ‘ä»¬åœ¨åˆå§‹åŒ– data loader çš„æ—¶å€™éœ€è¦ä½¿ç”¨åˆ° torch.utils.data.distributed.DistributedSampler

train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=..., sampler=train_sampler)
1.
2.
3.
 è¿™æ ·å°±èƒ½ç»™æ¯ä¸ªè¿›ç¨‹ä¸€ä¸ªä¸åŒçš„ samplerï¼Œå‘Šè¯‰æ¯ä¸ªè¿›ç¨‹è‡ªå·±åˆ†åˆ«å–å“ªäº›æ•°æ®ã€‚

ï¼ˆ3ï¼‰æ¨¡å‹çš„åˆå§‹åŒ–

å’Œ nn.DataParallel

model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank])
1.
ä½¿ç”¨ DistributedDataParallel åŒ…è£…æ¨¡å‹ï¼Œå®ƒèƒ½å¸®åŠ©æˆ‘ä»¬ä¸ºä¸åŒ GPU ä¸Šæ±‚å¾—çš„æ¢¯åº¦è¿›è¡Œ all reduceï¼ˆå³æ±‡æ€»ä¸åŒ GPU è®¡ç®—æ‰€å¾—çš„æ¢¯åº¦ï¼Œå¹¶åŒæ­¥è®¡ç®—ç»“æœï¼‰ã€‚all reduce åä¸åŒ GPU ä¸­æ¨¡å‹çš„æ¢¯åº¦å‡ä¸º all reduce ä¹‹å‰å„ GPU æ¢¯åº¦çš„å‡å€¼ã€‚

ï¼ˆ4ï¼‰åŒæ­¥æ‰¹å½’ä¸€åŒ–ï¼ˆ ğ’”ğ’šğ’ğ’„ğ’‰ğ’“ğ’ğ’ğ’Šğ’›ğ’†ğ’… ğ‘©ğ’‚ğ’•ğ’„ğ’‰ ğ‘µğ’ğ’“ğ’ğ’‚ğ’ğ’Šğ’›ğ’‚ğ’•ğ’Šğ’ğ’ ï¼‰

ä¸ºä½•è¦è¿›è¡ŒåŒæ­¥BNå‘¢ï¼Ÿç°æœ‰çš„æ ‡å‡† Batch Normalization å› ä¸ºä½¿ç”¨æ•°æ®å¹¶è¡Œï¼ˆData Parallelï¼‰ï¼Œæ˜¯å•å¡çš„å®ç°æ¨¡å¼ï¼Œåªå¯¹å•ä¸ªå¡ä¸Šå¯¹æ ·æœ¬è¿›è¡Œå½’ä¸€åŒ–ï¼Œç›¸å½“äºå‡å°äº†æ‰¹é‡å¤§å°ï¼ˆbatch-sizeï¼‰ã€‚å¯¹äºæ¯”è¾ƒæ¶ˆè€—æ˜¾å­˜çš„è®­ç»ƒä»»åŠ¡æ—¶ï¼Œå¾€å¾€å•å¡ä¸Šçš„ç›¸å¯¹æ‰¹é‡è¿‡å°ï¼Œå½±å“æ¨¡å‹çš„æ”¶æ•›æ•ˆæœã€‚ä¹‹å‰åœ¨æˆ‘ä»¬åœ¨å›¾åƒè¯­ä¹‰åˆ†å‰²çš„å®éªŒä¸­ï¼ŒJerryå’Œæˆ‘å°±å‘ç°ä½¿ç”¨å¤§æ¨¡å‹çš„æ•ˆæœåè€Œå˜å·®ï¼Œå®é™…ä¸Šå°±æ˜¯BNåœ¨ä½œæ€ªã€‚è·¨å¡åŒæ­¥ Batch Normalization å¯ä»¥ä½¿ç”¨å…¨å±€çš„æ ·æœ¬è¿›è¡Œå½’ä¸€åŒ–ï¼Œè¿™æ ·ç›¸å½“äºâ€˜å¢å¤§â€˜äº†æ‰¹é‡å¤§å°ï¼Œè¿™æ ·è®­ç»ƒæ•ˆæœä¸å†å—åˆ°ä½¿ç”¨ GPU æ•°é‡çš„å½±å“ã€‚æœ€è¿‘åœ¨å›¾åƒåˆ†å‰²ã€ç‰©ä½“æ£€æµ‹çš„è®ºæ–‡ä¸­ï¼Œä½¿ç”¨è·¨å¡BNä¹Ÿä¼šæ˜¾è‘—åœ°æé«˜å®éªŒæ•ˆæœï¼Œæ‰€ä»¥è·¨å¡ BN å·²ç„¶æˆä¸ºç«èµ›åˆ·åˆ†ã€å‘è®ºæ–‡çš„å¿…å¤‡ç¥å™¨ã€‚

å¯æƒœ PyTorch å¹¶æ²¡æœ‰ä¸ºæˆ‘ä»¬å®ç°è¿™ä¸€åŠŸèƒ½ï¼Œåœ¨æ¥ä¸‹æ¥çš„ä»‹ç»ä¸­æˆ‘ä»¬ä¼šåœ¨ apex ä¸­çœ‹åˆ°è¿™ä¸€åŠŸèƒ½ã€‚

2.3 æ±‡æ€»
è‡³æ­¤ï¼Œæˆ‘ä»¬å°±å¯ä»¥ä½¿ç”¨ torch.distributed åŠnn.DistributedDataParallelç»™æˆ‘ä»¬å¸¦æ¥çš„å¤šè¿›ç¨‹è®­ç»ƒçš„æ€§èƒ½æå‡äº†ï¼Œæ±‡æ€»ä»£ç ç»“æœå¦‚ä¸‹ï¼š

# main.py
import torch
import argparse
import torch.distributed as dist

parser = argparse.ArgumentParser()
parser.add_argument('--local_rank', default=-1, type=int,
                    help='node rank for distributed training')
args = parser.parse_args()

dist.init_process_group(backend='nccl')
torch.cuda.set_device(args.local_rank)

train_dataset = ...
#æ¯ä¸ªè¿›ç¨‹ä¸€ä¸ªsampler
train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=..., sampler=train_sampler)

model = ...
model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank])

optimizer = optim.SGD(model.parameters())

for epoch in range(100):
   for batch_idx, (data, target) in enumerate(train_loader):
      images = images.cuda(non_blocking=True)
      target = target.cuda(non_blocking=True)
      ...
      output = model(images)
      loss = criterion(output, target)
      ...
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.
17.
18.
19.
20.
21.
22.
23.
24.
25.
26.
27.
28.
29.
30.
31.
32.
33.
34.
35.
åœ¨ä½¿ç”¨æ—¶ï¼Œè°ƒç”¨ torch.distributed.launch å¯åŠ¨å™¨å¯åŠ¨ï¼š

CUDA_VISIBLE_DEVICES=0,1,2,3 python -m torch.distributed.launch --nproc_per_node=4 main.py
1.
åœ¨ ImageNet ä¸Šçš„å®Œæ•´è®­ç»ƒä»£ç ï¼Œè¯·ç‚¹å‡»ï¼š pytorch-distributed/distributed.py at master Â· tczhangzhi/pytorch-distributed Â· GitHub

 3. ä½¿ç”¨ apex å†åŠ é€Ÿ(æ··åˆç²¾åº¦è®­ç»ƒã€å¹¶è¡Œè®­ç»ƒã€åŒæ­¥BN)
3.1 apexä»‹ç»
æ³¨ï¼šéœ€è¦ä½¿ç”¨åˆ°Voltaç»“æ„çš„GPUï¼Œç›®å‰åªæœ‰Tesla V100å’ŒTITAN Vç³»åˆ—æ”¯æŒã€‚

 Apex æ˜¯ NVIDIA å¼€æºçš„ç”¨äºæ··åˆç²¾åº¦è®­ç»ƒå’Œåˆ†å¸ƒå¼è®­ç»ƒåº“ã€‚Apex å¯¹æ··åˆç²¾åº¦è®­ç»ƒçš„è¿‡ç¨‹è¿›è¡Œäº†å°è£…ï¼Œæ”¹ä¸¤ä¸‰è¡Œé…ç½®å°±å¯ä»¥è¿›è¡Œæ··åˆç²¾åº¦çš„è®­ç»ƒï¼Œä»è€Œå¤§å¹…åº¦é™ä½æ˜¾å­˜å ç”¨ï¼ŒèŠ‚çº¦è¿ç®—æ—¶é—´ã€‚æ­¤å¤–ï¼ŒApex ä¹Ÿæä¾›äº†å¯¹åˆ†å¸ƒå¼è®­ç»ƒçš„å°è£…ï¼Œé’ˆå¯¹ NVIDIA çš„ NCCL é€šä¿¡åº“è¿›è¡Œäº†ä¼˜åŒ–ã€‚

æ··åˆç²¾åº¦è®­ç»ƒæ˜¯åœ¨å°½å¯èƒ½å‡å°‘ç²¾åº¦æŸå¤±çš„æƒ…å†µä¸‹åˆ©ç”¨åŠç²¾åº¦æµ®ç‚¹æ•°åŠ é€Ÿè®­ç»ƒã€‚å®ƒä½¿ç”¨FP16å³åŠç²¾åº¦æµ®ç‚¹æ•°å­˜å‚¨æƒé‡å’Œæ¢¯åº¦ã€‚åœ¨å‡å°‘å ç”¨å†…å­˜çš„åŒæ—¶èµ·åˆ°äº†åŠ é€Ÿè®­ç»ƒçš„æ•ˆæœã€‚æ€»ç»“ä¸‹æ¥å°±æ˜¯ä¸¤ä¸ªåŸå› ï¼šå†…å­˜å ç”¨æ›´å°‘ï¼Œè®¡ç®—æ›´å¿«ã€‚

å†…å­˜å ç”¨æ›´å°‘ï¼šè¿™ä¸ªæ˜¯æ˜¾ç„¶å¯è§çš„ï¼Œé€šç”¨çš„æ¨¡å‹ fp16 å ç”¨çš„å†…å­˜åªéœ€åŸæ¥çš„ä¸€åŠã€‚memory-bandwidth å‡åŠæ‰€å¸¦æ¥çš„å¥½å¤„ï¼š
æ¨¡å‹å ç”¨çš„å†…å­˜æ›´å°ï¼Œè®­ç»ƒçš„æ—¶å€™å¯ä»¥ç”¨æ›´å¤§çš„batchsizeã€‚æ¨¡å‹è®­ç»ƒæ—¶ï¼Œé€šä¿¡é‡ï¼ˆç‰¹åˆ«æ˜¯å¤šå¡ï¼Œæˆ–è€…å¤šæœºå¤šå¡ï¼‰å¤§å¹…å‡å°‘ï¼Œå¤§å¹…å‡å°‘ç­‰å¾…æ—¶é—´ï¼ŒåŠ å¿«æ•°æ®çš„æµé€šã€‚
è®¡ç®—æ›´å¿«
ç›®å‰çš„ä¸å°‘GPUéƒ½æœ‰é’ˆå¯¹ fp16 çš„è®¡ç®—è¿›è¡Œä¼˜åŒ–ã€‚è®ºæ–‡æŒ‡å‡ºï¼šåœ¨è¿‘æœŸçš„GPUä¸­ï¼ŒåŠç²¾åº¦çš„è®¡ç®—ååé‡å¯ä»¥æ˜¯å•ç²¾åº¦çš„ 2-8 å€ï¼›ä»ä¸‹å›¾æˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ··åˆç²¾åº¦è®­ç»ƒå‡ ä¹æ²¡æœ‰æ€§èƒ½æŸå¤±ã€‚
3.2 ä½¿ç”¨æ–¹å¼
ï¼ˆ1ï¼‰æ··åˆç²¾åº¦

åœ¨æ··åˆç²¾åº¦è®­ç»ƒä¸Šï¼ŒApex çš„å°è£…ååˆ†ä¼˜é›…ã€‚ç›´æ¥ä½¿ç”¨ amp.initialize

from apex import amp

model, optimizer = amp.initialize(model, optimizer, opt_level='O1')
1.
2.
3.
å…¶ä¸­ opt_level ä¸ºç²¾åº¦çš„ä¼˜åŒ–è®¾ç½®ï¼ŒO0ï¼ˆç¬¬ä¸€ä¸ªå­—æ¯æ˜¯å¤§å†™å­—æ¯Oï¼‰ï¼š

O0ï¼šçº¯FP32è®­ç»ƒï¼Œå¯ä»¥ä½œä¸ºaccuracyçš„baselineï¼›
O1ï¼šæ··åˆç²¾åº¦è®­ç»ƒï¼ˆæ¨èä½¿ç”¨ï¼‰ï¼Œæ ¹æ®é»‘ç™½åå•è‡ªåŠ¨å†³å®šä½¿ç”¨FP16ï¼ˆGEMM, å·ç§¯ï¼‰è¿˜æ˜¯FP32ï¼ˆSoftmaxï¼‰è¿›è¡Œè®¡ç®—ã€‚
O2ï¼šâ€œå‡ ä¹FP16â€æ··åˆç²¾åº¦è®­ç»ƒï¼Œä¸å­˜åœ¨é»‘ç™½åå•ï¼Œé™¤äº†Batch normï¼Œå‡ ä¹éƒ½æ˜¯ç”¨FP16è®¡ç®—ã€‚
O3ï¼šçº¯FP16è®­ç»ƒï¼Œå¾ˆä¸ç¨³å®šï¼Œä½†æ˜¯å¯ä»¥ä½œä¸ºspeedçš„baselineï¼›
ï¼ˆ2ï¼‰ å¹¶è¡Œè®­ç»ƒ

Apexä¹Ÿå®ç°äº†å¹¶è¡Œè®­ç»ƒæ¨¡å‹çš„è½¬æ¢æ–¹å¼ï¼Œæ”¹åŠ¨å¹¶ä¸å¤§ï¼Œä¸»è¦æ˜¯ä¼˜åŒ–äº†NCCLçš„é€šä¿¡ï¼Œå› æ­¤ä»£ç å’Œ torch.distributed

from apex import amp
from apex.parallel import DistributedDataParallel

model, optimizer = amp.initialize(model, optimizer, opt_level='O1')
model = DistributedDataParallel(model, delay_allreduce=True)

# åå‘ä¼ æ’­æ—¶éœ€è¦è°ƒç”¨ amp.scale_lossï¼Œç”¨äºæ ¹æ®losså€¼è‡ªåŠ¨å¯¹ç²¾åº¦è¿›è¡Œç¼©æ”¾
with amp.scale_loss(loss, optimizer) as scaled_loss:
    scaled_loss.backward()
1.
2.
3.
4.
5.
6.
7.
8.
9.
ï¼ˆ3ï¼‰åŒæ­¥BN

Apexä¸ºæˆ‘ä»¬å®ç°äº†åŒæ­¥BNï¼Œç”¨äºè§£å†³å•GPUçš„minibatchå¤ªå°å¯¼è‡´BNåœ¨è®­ç»ƒæ—¶ä¸æ”¶æ•›çš„é—®é¢˜ã€‚

from apex.parallel import convert_syncbn_model
from apex.parallel import DistributedDataParallel

# æ³¨æ„é¡ºåºï¼šä¸‰ä¸ªé¡ºåºä¸èƒ½é”™
model = convert_syncbn_model(UNet3d(n_channels=1, n_classes=1)).to(device)
model, optimizer = amp.initialize(model, optimizer, opt_level='O1')
model = DistributedDataParallel(model, delay_allreduce=True)
1.
2.
3.
4.
5.
6.
7.
è°ƒç”¨è¯¥å‡½æ•°åï¼ŒApexä¼šè‡ªåŠ¨éå†modelçš„æ‰€æœ‰å±‚ï¼Œå°†BatchNormå±‚æ›¿æ¢æ‰ã€‚

3.3 æ±‡æ€»
Apexçš„å¹¶è¡Œè®­ç»ƒéƒ¨åˆ†ä¸»è¦ä¸å¦‚ä¸‹ä»£ç æ®µæœ‰å…³ï¼š

# main.py
import torch
import argparse
import torch.distributed as dist

from apex.parallel import convert_syncbn_model
from apex.parallel import DistributedDataParallel

parser = argparse.ArgumentParser()
parser.add_argument('--local_rank', default=-1, type=int,
                    help='node rank for distributed training')
args = parser.parse_args()

dist.init_process_group(backend='nccl')
torch.cuda.set_device(args.local_rank)

train_dataset = ...
train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=..., sampler=train_sampler)

model = ...
#åŒæ­¥BN
model = convert_syncbn_model(model)
#æ··åˆç²¾åº¦
model, optimizer = amp.initialize(model, optimizer)
#åˆ†å¸ƒæ•°æ®å¹¶è¡Œ
model = DistributedDataParallel(model, device_ids=[args.local_rank])

optimizer = optim.SGD(model.parameters())

for epoch in range(100):
   for batch_idx, (data, target) in enumerate(train_loader):
      images = images.cuda(non_blocking=True)
      target = target.cuda(non_blocking=True)
      ...
      output = model(images)
      loss = criterion(output, target)
      optimizer.zero_grad()
      with amp.scale_loss(loss, optimizer) as scaled_loss:
         scaled_loss.backward()
      optimizer.step()
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.
17.
18.
19.
20.
21.
22.
23.
24.
25.
26.
27.
28.
29.
30.
31.
32.
33.
34.
35.
36.
37.
38.
39.
40.
41.
42.
ä½¿ç”¨ launch å¯åŠ¨ï¼š

CUDA_VISIBLE_DEVICES=0,1,2,3 python -m torch.distributed.launch --nproc_per_node=4 main.py
1.
 4 å¤šå¡è®­ç»ƒæ—¶çš„æ•°æ®è®°å½•ï¼ˆTensorBoardã€torch.saveï¼‰
4.1 è®°å½•Lossæ›²çº¿
åœ¨æˆ‘ä»¬ä½¿ç”¨å¤šè¿›ç¨‹æ—¶ï¼Œæ¯ä¸ªè¿›ç¨‹æœ‰è‡ªå·±è®¡ç®—å¾—åˆ°çš„Lossï¼Œæˆ‘ä»¬åœ¨è¿›è¡Œæ•°æ®è®°å½•æ—¶ï¼Œå¸Œæœ›å¯¹ä¸åŒè¿›ç¨‹ä¸Šçš„Losså–å¹³å‡ï¼ˆä¹Ÿå°±æ˜¯ map-reduce çš„åšæ³•ï¼‰ï¼Œå¯¹äºå…¶ä»–éœ€è¦è®°å½•çš„æ•°æ®ä¹Ÿéƒ½æ˜¯ä¸€æ ·çš„åšæ³•ï¼š

def reduce_tensor(tensor):
    # Reduces the tensor data across all machines
    # If we print the tensor, we can get:
    # tensor(334.4330, device='cuda:1') *********************, here is cuda:  cuda:1
    # tensor(359.1895, device='cuda:3') *********************, here is cuda:  cuda:3
    # tensor(263.3543, device='cuda:2') *********************, here is cuda:  cuda:2
    # tensor(340.1970, device='cuda:0') *********************, here is cuda:  cuda:0
    rt = tensor.clone()  # The function operates in-place.
    dist.all_reduce(rt, op=dist.reduce_op.SUM)
    rt /= dist.get_world_size()#æ€»è¿›ç¨‹æ•°
    return rt

# calculate loss
loss = criterion(predict, labels)
reduced_loss = reduce_tensor(loss.data)
train_epoch_loss += reduced_loss.item()

'''
æ³¨æ„åœ¨å†™å…¥TensorBoardçš„æ—¶å€™åªè®©ä¸€ä¸ªè¿›ç¨‹å†™å…¥å°±å¤Ÿäº†ï¼š
'''

# TensorBoard
if args.local_rank == 0:
    writer.add_scalars('Loss/training', {
        'train_loss': train_epoch_loss,
        'val_loss': val_epoch_loss
    }, epoch + 1)
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.
17.
18.
19.
20.
21.
22.
23.
24.
25.
26.
27.
4.2 torch.saveä¿å­˜æ¨¡å‹
åœ¨ä¿å­˜æ¨¡å‹çš„æ—¶å€™ï¼Œç”±äºæ˜¯Apexæ··åˆç²¾åº¦æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨Apexæä¾›çš„ä¿å­˜ã€è½½å…¥æ–¹æ³•ï¼ˆè§Apex READMEï¼‰ï¼š

# Save checkpoint
checkpoint = {
    'model': model.state_dict(),
    'optimizer': optimizer.state_dict(),
    'amp': amp.state_dict()
}
torch.save(checkpoint, 'amp_checkpoint.pt')
...

# Restore
model = ...
optimizer = ...
checkpoint = torch.load('amp_checkpoint.pt')

model, optimizer = amp.initialize(model, optimizer, opt_level=opt_level)
model.load_state_dict(checkpoint['model'])
optimizer.load_state_dict(checkpoint['optimizer'])
amp.load_state_dict(checkpoint['amp'])

# Continue training
...
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.
17.
18.
19.
20.
21.
 5 å¤šå¡åçš„ batch_size å’Œ learning_rate çš„è°ƒæ•´
çŸ¥ä¹ï¼šã€Š å¦‚ä½•ç†è§£æ·±åº¦å­¦ä¹ åˆ†å¸ƒå¼è®­ç»ƒä¸­çš„large batch sizeä¸learning rateçš„å…³ç³»ï¼Ÿã€‹

ä»ç†è®ºä¸Šæ¥è¯´ï¼Œlr = batch_size * base lrï¼Œå› ä¸º batch_size çš„å¢å¤§ä¼šå¯¼è‡´ä½  update æ¬¡æ•°çš„å‡å°‘ï¼Œæ‰€ä»¥ä¸ºäº†è¾¾åˆ°ç›¸åŒçš„æ•ˆæœï¼Œåº”è¯¥æ˜¯åŒæ¯”ä¾‹å¢å¤§çš„ã€‚ä½†æ˜¯æ›´å¤§çš„ lr å¯èƒ½ä¼šå¯¼è‡´æ”¶æ•›çš„ä¸å¤Ÿå¥½ï¼Œå°¤å…¶æ˜¯åœ¨åˆšå¼€å§‹çš„æ—¶å€™ï¼Œå¦‚æœä½ ä½¿ç”¨å¾ˆå¤§çš„ lrï¼Œå¯èƒ½ä¼šç›´æ¥çˆ†ç‚¸ï¼Œæ‰€ä»¥å¯èƒ½ä¼šéœ€è¦ä¸€äº› warmup æ¥é€æ­¥çš„æŠŠ lr æé«˜åˆ°ä½ æƒ³è®¾å®šçš„ lrã€‚å®é™…åº”ç”¨ä¸­å‘ç°ä¸ä¸€å®šè¦åŒæ¯”ä¾‹å¢é•¿ï¼Œæœ‰æ—¶å€™å¯èƒ½å¢å¤§åˆ° batch_size/2 å€çš„æ•ˆæœå·²ç»å¾ˆä¸é”™äº†ã€‚åœ¨æˆ‘çš„å®éªŒä¸­ï¼Œä½¿ç”¨8å¡è®­ç»ƒï¼Œåˆ™å¢å¤§batch_size 8å€ï¼Œlearning_rate 4å€æ˜¯å·®ä¸å¤šçš„ã€‚

 6 å®Œæ•´ä»£ç ç¤ºä¾‹ï¼ˆApexæ··åˆç²¾åº¦çš„Distributed DataParallelï¼Œç”¨æ¥è®­ç»ƒ3D U-Netçš„ï¼‰
import os
import datetime
import argparse
from tqdm import tqdm
import torch
from torch import distributed, optim
from torch.utils.data import DataLoader
#æ¯ä¸ªè¿›ç¨‹ä¸åŒsampler
from torch.utils.data.distributed import DistributedSampler
from torch.utils.tensorboard import SummaryWriter
#æ··åˆç²¾åº¦
from apex import amp
#åŒæ­¥BN
from apex.parallel import convert_syncbn_model
#Distributed DataParallel
from apex.parallel import DistributedDataParallel

from models import UNet3d
from datasets import IronGrain3dDataset
from losses import BCEDiceLoss
from eval import eval_net

train_images_folder = '../../datasets/IronGrain/74x320x320/train_patches/images/'
train_labels_folder = '../../datasets/IronGrain/74x320x320/train_patches/labels/'
val_images_folder = '../../datasets/IronGrain/74x320x320/val_patches/images/'
val_labels_folder = '../../datasets/IronGrain/74x320x320/val_patches/labels/'


def parse():
    parser = argparse.ArgumentParser()
    parser.add_argument('--local_rank', type=int, default=0)
    args = parser.parse_args()
    return args


def main():
    args = parse()

#è®¾ç½®å½“å‰è¿›ç¨‹çš„deviceï¼ŒGPUé€šä¿¡æ–¹å¼ä¸ºNCCL
    torch.cuda.set_device(args.local_rank)
    distributed.init_process_group(
        'nccl',
        init_method='env://'
    )

#åˆ¶ä½œDatasetå’Œsampler
    train_dataset = IronGrain3dDataset(train_images_folder, train_labels_folder)
    val_dataset = IronGrain3dDataset(val_images_folder, val_labels_folder)
    train_sampler = DistributedSampler(train_dataset)
    val_sampler = DistributedSampler(val_dataset)

    epochs = 100
    batch_size = 8
    lr = 2e-4
    weight_decay = 1e-4
    device = torch.device(f'cuda:{args.local_rank}')

#åˆ¶ä½œDataLoader
    train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=4,
                              pin_memory=True, sampler=train_sampler)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4,
                            pin_memory=True, sampler=val_sampler)

#3æ­¥æ›²ï¼šåŒæ­¥BNï¼Œåˆå§‹åŒ–ampï¼ŒDistributedDataParallelå°è£…
    net = convert_syncbn_model(UNet3d(n_channels=1, n_classes=1)).to(device)
    optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)
    net, optimizer = amp.initialize(net, optimizer, opt_level='O1')
    net = DistributedDataParallel(net, delay_allreduce=True)
    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[25, 50, 75], gamma=0.2)
    criterion = BCEDiceLoss().to(device)

    if args.local_rank == 0:
        print(f'''Starting training:
            Epochs:          {epochs}
            Batch size:      {batch_size}
            Learning rate:   {lr}
            Training size:   {len(train_dataset)}
            Validation size: {len(val_dataset)}
            Device:          {device.type}
        ''')
        writer = SummaryWriter(
            log_dir=f'runs/irongrain/unet3d_32x160x160_BS_{batch_size}_{datetime.datetime.now()}'
        )
    for epoch in range(epochs):
        train_epoch_loss = 0
        with tqdm(total=len(train_dataset), desc=f'Epoch {epoch + 1}/{epochs}', unit='img') as pbar:

            images = None
            labels = None
            predict = None

            # train
            net.train()
            for batch_idx, batch in enumerate(train_loader):
                images = batch['image']
                labels = batch['label']
                images = images.to(device, dtype=torch.float32)
                labels = labels.to(device, dtype=torch.float32)

                predict = net(images)

                # calculate loss
                # reduceä¸åŒè¿›ç¨‹çš„loss
                loss = criterion(predict, labels)
                reduced_loss = reduce_tensor(loss.data)
                train_epoch_loss += reduced_loss.item()

                # optimize
                optimizer.zero_grad()
                with amp.scale_loss(loss, optimizer) as scaled_loss:
                    scaled_loss.backward()
                optimizer.step()
                scheduler.step()

                # set progress bar
                pbar.set_postfix(**{'loss (batch)': loss.item()})
                pbar.update(images.shape[0])

            train_epoch_loss /= (batch_idx + 1)

            # eval
            val_epoch_loss, dice, iou = eval_net(net, criterion, val_loader, device, len(val_dataset))

            # TensorBoard
            if args.local_rank == 0:
                writer.add_scalars('Loss/training', {
                    'train_loss': train_epoch_loss,
                    'val_loss': val_epoch_loss
                }, epoch + 1)

                writer.add_scalars('Metrics/validation', {
                    'dice': dice,
                    'iou': iou
                }, epoch + 1)

                writer.add_images('images', images[:, :, 0, :, :], epoch + 1)
                writer.add_images('Label/ground_truth', labels[:, :, 0, :, :], epoch + 1)
                writer.add_images('Label/predict', torch.sigmoid(predict[:, :, 0, :, :]) > 0.5, epoch + 1)

            if args.local_rank == 0:
                torch.save(net, f'unet3d-epoch{epoch + 1}.pth')


def reduce_tensor(tensor: torch.Tensor) -> torch.Tensor:
    rt = tensor.clone()
    distributed.all_reduce(rt, op=distributed.reduce_op.SUM)
    rt /= distributed.get_world_size()#è¿›ç¨‹æ•°
    return rt


if __name__ == '__main__':
    main()

ä¸‰ã€å‚è€ƒ
ã€ŠPyTorch å•æœºå¤šå¡æ“ä½œæ€»ç»“ï¼šåˆ†å¸ƒå¼DataParallelï¼Œæ··åˆç²¾åº¦ï¼ŒHorovod)ã€‹
