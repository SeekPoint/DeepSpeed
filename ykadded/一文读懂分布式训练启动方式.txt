一文读懂分布式训练启动方式
https://zhuanlan.zhihu.com/p/675464874


一文读懂分布式训练启动方式
游凯超
游凯超​
深度学习（Deep Learning）话题下的优秀答主
已关注

你关注的 猛猿 赞同
关于分布式训练的原理，网络上已经有非常多精彩的解读了。但是关于分布式训练的启动方式却少有相关资料。或许这是因为，我们很少自己写启动脚本，大部分时候都是从网上找到一份README，然后照着README的说明直接用。为了深入理解分布式训练，我觉得还是有必要把分布式训练的启动方式做一个系统性梳理。

PyTorch的分布式训练启动方式
PyTorch的分布式训练主要有三种启动方式。我们以一个简单的示例（用N个进程来求
），来演示各种启动方式的区别：

1.手动使用torch.multiprocessing.spawn
# spawn.py

import torch
import torch.distributed as dist
import torch.multiprocessing as mp
import os

def worker(rank, world_size):
    dist.init_process_group("gloo", rank=rank, world_size=world_size)
    tensor = torch.tensor(rank + 1, dtype=torch.float)
    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)
    if rank == 0:
        print(f"Sum of ranks: {tensor.item()}")
    dist.destroy_process_group()

def main(world_size):
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '29500'
    mp.set_start_method('spawn')
    mp.spawn(worker, args=(world_size,), nprocs=world_size, join=True)

if __name__ == "__main__":
    main(4)  # Number of processes
这种方式的优点是启动方式非常简单，只需要python spawn.py就可以执行。它的缺点也非常明显，我们必须在代码里手动调用mp.spawn来生成多个进程，而且总进程数4、通信地址localhost:29500都是写死在代码里的。另外，它的代码不直观，在mp.spawn里面的args=(world_size,)似乎表示函数只有一个参数，而实际上函数收到的参数为rank, world_size一共两个参数。

总的来说，这种启动方式是非常原始的分布式训练方式，代码里需要考虑的细节太多，目前并不建议使用。如果偶尔看到类似代码，应该是上古遗留代码了，可以考虑送去博物馆展览。

2.使用torch.distributed.launch
# launch.py

import torch
import torch.distributed as dist
import argparse
import os

def worker(rank, world_size):
    dist.init_process_group("gloo", rank=rank, world_size=world_size)
    tensor = torch.tensor(rank + 1, dtype=torch.float)
    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)
    if rank == 0:
        print(f"Sum of ranks: {tensor.item()}")
    dist.destroy_process_group()

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--local-rank", type=int)
    args = parser.parse_args()

    world_size = int(os.environ['WORLD_SIZE'])
    worker(args.local_rank, world_size)

if __name__ == "__main__":
    main()
这种方式的启动稍微复杂一些，代码为python -m torch.distributed.launch --nproc-per-node=4 launch.py，好处是我们不用把并行数目、通信端口等写在代码里，只需要在启动脚本里用--nproc-per-node指定即可。同时我们看到，这一参数的值在代码中可以通过WORLD_SIZE环境变量获取。

使用torch.distributed.launch的缺点在于我们必须使用argparse，而且必须有一个--local-rank参数。这种方式依然不够优雅，它作为一种分布式启动的方式，居然要求程序必须接受特定启动参数。

这种方式也即将过时，我们在运行时会看到一段警告FutureWarning: The module torch.distributed.launch is deprecated。如果看到有些代码的argparse有parser.add_argument("--local-rank", type=int)，那它大概率就是通过torch.distributed.launch启动的。如果它是别人写的代码，并且还能运行，那就捏着鼻子忍着用吧；如果是自己要写新的分布式启动代码，那最好还是用最新的推荐用法torchrun。

3.使用torchrun
新版本的PyTorch一般用torchrun来启动分布式训练。脚本里的代码示例：

# run.py
import torch
import torch.distributed as dist

def worker():
    dist.init_process_group("gloo")
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    tensor = torch.tensor(rank + 1, dtype=torch.float)
    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)
    if rank == 0:
        print(f"Sum of ranks: {tensor.item()}")
    dist.destroy_process_group()

def main():
    worker()

if __name__ == "__main__":
    main()
启动方式为：torchrun --nproc-per-node=4 run.py，相当简洁优雅。

这种方式有以下几个优点：

启动命令从python -m torch.distributed.launch变成torchrun，少打字，而且还能被zsh等shell补全，拯救程序员的指关节。
启动脚本与代码完全解耦，启动脚本对代码没有侵入式要求。实际上，启动脚本只是为每个进程设置了一些环境变量。
代码里的函数不必再接受rank, world_size参数。它们可以从dist.get_rank()/dist.get_world_size()获取，而不必作为函数参数到处传递。
缺点只有一个：需要足够高的PyTorch版本。然而，PyTorch版本的更新基本不会破坏兼容性，为什么不升级呢？又不像TensorFlow，小版本小变化、大版本大变化（日常喷TensorFlow）。

下面的例子中，我们将采用torchrun作为默认启动方式。

PyTorch的多机分布式训练启动方式
以上代码中，我们使用--nproc-per-node=4来指定并行数目。这是在单台机器上进行并行的脚本。如果我们要在多台机器上并行，则启动方式要稍微复杂一些。

多台机器一起并行训练的第一步，是需要相互通信。我们需要指定一台机器作为控制中心，也就是确定MASTER_ADDR参数。

当我们在一些云服务平台上提交分布式训练的任务时，云平台一般都会帮我们设定一些元数据，其中就会包括当前分布式任务中所有机器的ip地址等信息，一些高级的云平台也可以直接为我们指定一台机器作为控制中心。如果我们的多台机器不是云平台分配的而是一直运行的多台物理机，那我们需要保证每台机器都能与MASTER_ADDR那台机器通信。

准备工作：确保所有机器上的软件版本一致
云平台启动分布式任务时常常要求声明启动镜像，因此所有机器上的软件版本基本是一致的。这一要求主要针对物理机，如果多台机器的PyTorch版本不同、操作系统版本不同，各种错误都可能出现。

准备工作：确保机器之间能通信
这里以多台物理机为例，展示如何确认机器之间能够互相通信：

在主机器（任选一台作为主机器）上运行：python -m http.server --bind 0.0.0.0 8000

在所有的机器（包括主机器）上运行：wget http://$MASTER_ADDR:8000 ，确保这条命令在所有的机器上都能正确运行。如果不行，先调试网络问题（尤其是网络代理的问题）。

如果是云平台分配的机器，可以查阅云平台的文档，查看如何确定主机器，然后将主机器的ip设置到对应环境变量：export MASTER_ADDR=$PLATFORM_MASTER_ADDR。

设置使用的网口：如果机器有多根网线、网络配置复杂，还需要设置对应的网口，例如export GLOO_SOCKET_IFNAME=enp13s0f0。这个网口应该对应于这些机器能够互相访问到的网段。如果不知道，咨询管理员。（如果使用的是nccl作为分布式后端，则应该设置对应的NCCL_SOCKET_IFNAME；如果使用的是RoCE或者Infiniband之类的高速通信硬件，这个参数尤其重要）

启动脚本
在每台机器上（这里以两台为例）执行下列脚本： torchrun --nnodes 2 --nproc-per-node=4 --rdzv_backend=c10d --rdzv_endpoint=$MASTER_ADDR a.py

这里的高级参数--rdzv_backend=c10d可以节省很多复杂的配置，比如某些教程里会提到需要手动为每台机器分配不同的--node-rank、比如需要手动选择一个空闲的端口…… 当我们使用c10d时都不再需要了。唯一需要的就是指定主机器的ip，而且是放在--rdzv_endpoint参数里，不是--master-addr。

关于torchrun的进一步说明：实际上这是一个相当复杂的命令，输入torchrun -h就能看到，它有非常多的参数。它能够用于超大规模训练（例如成百上千台机器），还能自动出错重启、动态扩展参加训练的机器数目。这些高级功能默认都是关闭的，想要使用的朋友请参阅具体文档。

关于参数名中的rdzv，它是rendezvous的缩写，意思是“汇聚点”，大概可以理解为如何动态协调多台机器汇聚在一起协作。
注意事项
分布式脚本一旦启动，很难清理干净。物理机多机训练可能经常出现上一次的分布式脚本没有正常退出导致端口被占用的问题。

珍爱生命，保护头发。调试经常选择单机环境，多机分布式训练最好还是用云平台分配好的统一的机器（一般为容器机器，随便搞，大不了删了重新分配个容器），不要用几台物理机（天知道这些机器上面有什么古老的网络配置或者软件配置导致你的分布式训练无法启动）。
关于JAX的分布式训练启动方式
这里再简单提及JAX的分布式训练：JAX通常只在一台机器上启动一个进程，多台机器的话只需要在每台机器上启动一个进程就行了，因此没有启动脚本的概念，一般就是python code.py，代码里加上jax.distributed.initialize()，非常清爽，没有什么别的配置。

具体实现原理上，JAX也是用类似环境变量的方式来初始化的，它与mpi的配合较好。

在JAX的主力使用平台（TPU）上，它借助gcp的元数据服务器来获取全部机器的ip、配置等信息，具体可参见这部分的代码。

JAX在分布式计算上面的优势，也是一部分人选择JAX的原因之一。

分布式训练的基本概念
成功启动了分布式训练的脚本后，我们就可以开始研究分布式训练的细节了。

除了上文提到的dist.get_rank()/dist.get_world_size()，多机分布式训练将增加两个环境变量：LOCAL_WORLD_SIZE与LOCAL_RANK，分别表示本台机器的进程数、进程在本台机器的序号。

GPU分布式训练
以上脚本启动的是CPU上的分布式训练（具体来说，分布式通信操作dist.all_reduce是通过CPU来计算的），采用的通信后端是gloo。对于GPU操作，我们一般采用nccl作为后端，对应的修改为dist.init_process_group("nccl")。

对于PyTorch的GPU分布式训练来说，一般采用一个进程一张卡的方式启动，此时--nproc-per-node/LOCAL_WORLD_SIZE都是一台机器上的GPU数量（4或者8）。每一个进程使用GPU时，应该使用LOCAL_RANK作为GPU的序号。

分布式训练的调试方法
由于分布式训练是多个进程共同执行任务，基本上很难用调试器单步调试（一个进程经常会等待其它进程、一个断点可能被多个进程同时击中）。最推荐的调试方法，还是输出足够多的信息到日志文件里，然后对着日志文件找问题。

发布于 2023-12-31 22:13・IP 属地北京